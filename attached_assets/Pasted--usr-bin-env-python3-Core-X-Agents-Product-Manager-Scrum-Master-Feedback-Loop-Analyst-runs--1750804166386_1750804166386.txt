#!/usr/bin/env python3
"""
Core X-Agents: Product Manager ↔ Scrum Master Feedback Loop
Analyst runs once, then PM → Task Manager → Scrum Master cycles until approval
"""

import time
from lxml import etree
from abc import ABC, abstractmethod
import re

class BaseXAgent(ABC):
    """Base X-Agent with XML processing and performance tracking"""
    
    def __init__(self, agent_type: str):
        self.agent_type = agent_type
        self.metrics = {'total_time': 0}
    
    def process(self, input_xml: str) -> str:
        """Main processing with timing"""
        start_time = time.time()
        
        # Parse input
        parsed = etree.fromstring(input_xml.encode())
        
        # Process with agent intelligence
        result = self._process_intelligence(parsed)
        
        # Generate output XML
        output_xml = self._generate_xml(result)
        
        self.metrics['total_time'] = (time.time() - start_time) * 1000
        return output_xml
    
    @abstractmethod
    def _process_intelligence(self, parsed_input: etree.Element) -> dict:
        """Agent-specific processing logic"""
        pass
    
    @abstractmethod
    def _generate_xml(self, result: dict) -> str:
        """Generate XML output for next agent"""
        pass


class AnalystXAgent(BaseXAgent):
    """Analyzes documents and detects domain type (runs once)"""
    
    def __init__(self):
        super().__init__("AnalystXAgent")
    
    def _process_intelligence(self, parsed_input: etree.Element) -> dict:
        """Detect document domain and type"""
        content = etree.tostring(parsed_input, encoding='unicode', method='text').lower()
        
        # Simple domain detection
        if any(word in content for word in ['canvas', 'visual', 'workflow', 'drag']):
            domain = 'visual_workflow'
        elif any(word in content for word in ['enterprise', 'compliance', 'security']):
            domain = 'enterprise'
        else:
            domain = 'general'
        
        return {
            'domain': domain,
            'complexity': min(len(content) // 500, 5),
            'content': content[:2000]  # Keep original content for PM
        }
    
    def _generate_xml(self, result: dict) -> str:
        """Generate analysis XML for Product Manager"""
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<AnalysisPacket>
    <Domain>{result['domain']}</Domain>
    <Complexity>{result['complexity']}</Complexity>
    <Content>{result['content']}</Content>
</AnalysisPacket>"""


class ProductManagerXAgent(BaseXAgent):
    """Extracts requirements and identifies stakeholders (can process feedback)"""
    
    def __init__(self):
        super().__init__("ProductManagerXAgent")
    
    def _process_intelligence(self, parsed_input: etree.Element) -> dict:
        """Extract requirements - handles both initial analysis and feedback"""
        
        # Check if this is feedback from Scrum Master
        feedback_elem = parsed_input.find('Feedback')
        if feedback_elem is not None:
            return self._process_scrum_feedback(parsed_input)
        
        # Normal initial processing from Analyst
        domain = parsed_input.find('Domain').text
        content = parsed_input.find('Content').text
        
        return self._extract_requirements(domain, content)
    
    def _extract_requirements(self, domain: str, content: str, feedback_context: str = None) -> dict:
        """Extract requirements from content with optional feedback adjustments"""
        
        # Extract REQ-xxx patterns
        req_pattern = r'REQ-(\d+)[:\s]+(.*?)(?=\n|REQ-|\Z)'
        requirements = []
        
        for match in re.finditer(req_pattern, content, re.DOTALL):
            requirements.append({
                'id': f"REQ-{match.group(1).zfill(3)}",
                'title': match.group(2).strip()[:50],
                'priority': 'high' if 'critical' in match.group(2).lower() else 'medium'
            })
        
        # Apply feedback adjustments if provided
        if feedback_context:
            requirements = self._apply_feedback_to_requirements(requirements, feedback_context)
        
        # Basic stakeholder detection
        stakeholders = []
        if 'user' in content:
            stakeholders.append('End Users')
        if 'developer' in content:
            stakeholders.append('Development Team')
        if 'business' in content:
            stakeholders.append('Business Stakeholders')
        
        return {
            'domain': domain,
            'requirements': requirements,
            'stakeholders': stakeholders,
            'req_count': len(requirements),
            'feedback_applied': feedback_context is not None
        }
    
    def _process_scrum_feedback(self, feedback_input: etree.Element) -> dict:
        """Process feedback from Scrum Master and create updated PRD"""
        domain = feedback_input.find('Domain').text
        feedback = feedback_input.find('Feedback').text
        original_content = feedback_input.find('OriginalContent').text
        
        print(f"  [Product Manager] Processing Scrum Master feedback: {feedback}")
        
        # Apply feedback to create updated requirements
        return self._extract_requirements(domain, original_content, feedback)
    
    def _apply_feedback_to_requirements(self, requirements: list, feedback: str) -> list:
        """Apply Scrum Master feedback to adjust requirements"""
        
        if 'reduce scope' in feedback.lower():
            # Keep only high priority requirements
            high_priority_reqs = [req for req in requirements if req['priority'] == 'high']
            print(f"    → Reducing scope: {len(requirements)} → {len(high_priority_reqs)} requirements")
            return high_priority_reqs[:5]  # Max 5 high priority
            
        elif 'too complex' in feedback.lower():
            # Simplify requirement titles and reduce count
            simplified_reqs = []
            for i, req in enumerate(requirements[:6]):  # Limit to 6
                simplified_reqs.append({
                    'id': req['id'],
                    'title': f"Basic {req['title'][:30]}",  # Simplify titles
                    'priority': 'medium'  # Lower priority
                })
            print(f"    → Simplifying complexity: {len(requirements)} → {len(simplified_reqs)} simplified requirements")
            return simplified_reqs
            
        elif 'insufficient quality' in feedback.lower():
            # Add quality assurance requirements
            enhanced_reqs = requirements[:4]  # Reduce base requirements
            for req in enhanced_reqs:
                # Add quality sub-requirement
                enhanced_reqs.append({
                    'id': f"{req['id']}-QA",
                    'title': f"Quality Testing for {req['title'][:25]}",
                    'priority': 'high'
                })
            print(f"    → Adding quality focus: {len(requirements)} → {len(enhanced_reqs)} requirements with QA")
            return enhanced_reqs
            
        elif 'too many tasks' in feedback.lower():
            # Significantly reduce requirements
            core_reqs = requirements[:3]  # Only top 3
            print(f"    → Major scope reduction: {len(requirements)} → {len(core_reqs)} core requirements")
            return core_reqs
            
        else:
            # Default: moderate reduction
            moderate_reqs = requirements[:7]
            print(f"    → Moderate adjustment: {len(requirements)} → {len(moderate_reqs)} requirements")
            return moderate_reqs
    
    def _generate_xml(self, result: dict) -> str:
        """Generate requirements XML for Task Manager"""
        reqs_xml = '\n'.join([
            f'        <Requirement id="{req["id"]}" priority="{req["priority"]}">{req["title"]}</Requirement>'
            for req in result['requirements']
        ])
        
        stakeholders_xml = '\n'.join([
            f'        <Stakeholder>{stakeholder}</Stakeholder>'
            for stakeholder in result['stakeholders']
        ])
        
        feedback_note = "\n    <FeedbackApplied>true</FeedbackApplied>" if result['feedback_applied'] else ""
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<TaskPacket>
    <ProjectInfo>
        <Domain>{result['domain']}</Domain>
        <RequirementCount>{result['req_count']}</RequirementCount>
    </ProjectInfo>{feedback_note}
    <Requirements>
{reqs_xml}
    </Requirements>
    <Stakeholders>
{stakeholders_xml}
    </Stakeholders>
</TaskPacket>"""


class TaskManagerXAgent(BaseXAgent):
    """Breaks requirements into executable tasks (processes each PM iteration)"""
    
    def __init__(self):
        super().__init__("TaskManagerXAgent")
    
    def _process_intelligence(self, parsed_input: etree.Element) -> dict:
        """Generate tasks from requirements"""
        domain = parsed_input.find('.//Domain').text
        requirements = parsed_input.findall('.//Requirement')
        feedback_applied = parsed_input.find('FeedbackApplied') is not None
        
        if feedback_applied:
            print(f"  [Task Manager] Processing updated requirements ({len(requirements)} requirements)")
        
        tasks = []
        task_id = 1
        
        # Generate tasks for each requirement
        for req in requirements:
            req_id = req.get('id')
            req_title = req.text
            req_priority = req.get('priority', 'medium')
            
            # Standard task patterns per requirement
            patterns = ['Design', 'Implement', 'Test', 'Document']
            
            for pattern in patterns:
                # Adjust effort based on priority
                base_points = 3 if pattern == 'Implement' else 2
                base_hours = 12 if pattern == 'Implement' else 8
                
                if req_priority == 'high':
                    story_points = base_points + 1
                    hours = base_hours + 4
                else:
                    story_points = base_points
                    hours = base_hours
                
                tasks.append({
                    'id': f"TASK-{task_id:03d}",
                    'title': f"{pattern} {req_title[:30]}",
                    'req_id': req_id,
                    'story_points': story_points,
                    'hours': hours,
                    'priority': req_priority
                })
                task_id += 1
        
        # Add domain-specific tasks
        if domain == 'visual_workflow':
            tasks.extend([
                {'id': f"TASK-{task_id:03d}", 'title': 'Canvas Setup', 'req_id': 'DOMAIN', 'story_points': 5, 'hours': 16, 'priority': 'high'},
                {'id': f"TASK-{task_id+1:03d}", 'title': 'Drag & Drop', 'req_id': 'DOMAIN', 'story_points': 8, 'hours': 24, 'priority': 'high'}
            ])
        elif domain == 'enterprise':
            tasks.extend([
                {'id': f"TASK-{task_id:03d}", 'title': 'Security Setup', 'req_id': 'DOMAIN', 'story_points': 6, 'hours': 20, 'priority': 'high'},
                {'id': f"TASK-{task_id+1:03d}", 'title': 'Compliance Framework', 'req_id': 'DOMAIN', 'story_points': 8, 'hours': 28, 'priority': 'high'}
            ])
        
        total_story_points = sum(task['story_points'] for task in tasks)
        expansion_ratio = len(tasks) / max(len(requirements), 1)
        
        print(f"    → Generated {len(tasks)} tasks, {total_story_points} story points")
        
        return {
            'domain': domain,
            'tasks': tasks,
            'total_tasks': len(tasks),
            'story_points': total_story_points,
            'expansion_ratio': expansion_ratio,
            'req_count': len(requirements)
        }
    
    def _generate_xml(self, result: dict) -> str:
        """Generate task breakdown XML for Scrum Master"""
        tasks_xml = '\n'.join([
            f'        <Task id="{task["id"]}" req_id="{task["req_id"]}" points="{task["story_points"]}" hours="{task["hours"]}" priority="{task.get("priority", "medium")}">{task["title"]}</Task>'
            for task in result['tasks']
        ])
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<TaskBreakdown>
    <Summary>
        <Domain>{result['domain']}</Domain>
        <TotalTasks>{result['total_tasks']}</TotalTasks>
        <StoryPoints>{result['story_points']}</StoryPoints>
        <ExpansionRatio>{result['expansion_ratio']:.1f}x</ExpansionRatio>
        <RequirementCount>{result['req_count']}</RequirementCount>
    </Summary>
    <Tasks>
{tasks_xml}
    </Tasks>
</TaskBreakdown>"""


class POScrumMasterXAgent(BaseXAgent):
    """Final validation and release approval (generates feedback for PM)"""
    
    def __init__(self):
        super().__init__("POScrumMasterXAgent")
    
    def _process_intelligence(self, parsed_input: etree.Element) -> dict:
        """Validate project and approve/reject with specific feedback"""
        domain = parsed_input.find('.//Domain').text
        total_tasks = int(parsed_input.find('.//TotalTasks').text)
        story_points = int(parsed_input.find('.//StoryPoints').text)
        req_count = int(parsed_input.find('.//RequirementCount').text)
        
        print(f"  [Scrum Master] Reviewing: {total_tasks} tasks, {story_points} story points, {req_count} requirements")
        
        # Quality gates
        quality_checks = {
            'reasonable_task_count': total_tasks <= 50,
            'manageable_story_points': story_points <= 80,
            'adequate_scope': req_count >= 3,
            'good_task_ratio': (total_tasks / max(req_count, 1)) <= 15
        }
        
        quality_score = sum(quality_checks.values()) / len(quality_checks) * 100
        
        # Risk assessment
        if story_points > 100:
            risk = 'high'
        elif story_points > 60:
            risk = 'medium'
        else:
            risk = 'low'
        
        # Approval decision with specific feedback
        approved = quality_score >= 75 and risk != 'high'
        
        # Generate specific feedback for Product Manager
        feedback = self._generate_feedback(quality_checks, story_points, total_tasks, req_count, approved)
        
        return {
            'domain': domain,
            'approved': approved,
            'quality_score': quality_score,
            'risk_level': risk,
            'total_tasks': total_tasks,
            'story_points': story_points,
            'req_count': req_count,
            'feedback': feedback,
            'quality_checks': quality_checks
        }
    
    def _generate_feedback(self, quality_checks: dict, story_points: int, total_tasks: int, req_count: int, approved: bool) -> str:
        """Generate specific feedback for Product Manager"""
        
        if approved:
            return "APPROVED: Project scope and complexity are acceptable for execution"
        
        # Generate specific feedback based on what failed
        issues = []
        
        if story_points > 100:
            issues.append("too many story points - reduce scope significantly")
        elif story_points > 80:
            issues.append("reduce scope - story points too high")
        
        if total_tasks > 50:
            issues.append("too many tasks - simplify requirements")
        
        if not quality_checks['good_task_ratio']:
            issues.append("too complex - requirements generating too many tasks")
        
        if req_count > 8:
            issues.append("reduce scope - too many requirements")
        
        if not issues:
            issues.append("insufficient quality - improve requirements")
        
        return f"REJECTED: {', '.join(issues)}"
    
    def _generate_xml(self, result: dict) -> str:
        """Generate approval XML (with feedback if rejected)"""
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<ReleaseApproval>
    <Decision approved="{str(result['approved']).lower()}">
        <QualityScore>{result['quality_score']:.1f}%</QualityScore>
        <RiskLevel>{result['risk_level']}</RiskLevel>
        <TotalTasks>{result['total_tasks']}</TotalTasks>
        <StoryPoints>{result['story_points']}</StoryPoints>
        <RequirementCount>{result['req_count']}</RequirementCount>
    </Decision>
    <Feedback>{result['feedback']}</Feedback>
</ReleaseApproval>"""


class XAgentPipeline:
    """Pipeline with PM ↔ Scrum Master feedback loop"""
    
    def __init__(self):
        self.analyst = AnalystXAgent()
        self.product_manager = ProductManagerXAgent()
        self.task_manager = TaskManagerXAgent()
        self.scrum_master = POScrumMasterXAgent()
        self.max_iterations = 3
    
    def execute(self, document_content: str) -> dict:
        """Execute pipeline with PM-Scrum Master feedback loop"""
        
        print("🔍 Step 1: Document Analysis (runs once)")
        # Step 1: Analyst runs once
        document_xml = f"<?xml version='1.0'?><Document>{document_content}</Document>"
        analysis_xml = self.analyst.process(document_xml)
        
        print("📋 Step 2: Starting PM → Task Manager → Scrum Master cycle")
        # Step 2: PM → Task Manager → Scrum Master cycle
        current_input = analysis_xml
        iteration = 0
        
        while iteration < self.max_iterations:
            iteration += 1
            print(f"\n--- Iteration {iteration} ---")
            
            # Product Manager processes (initial analysis or feedback)
            print("📋 Product Manager: Creating/updating requirements")
            pm_output = self.product_manager.process(current_input)
            
            # Task Manager breaks down requirements
            print("🔧 Task Manager: Breaking down into tasks")
            task_output = self.task_manager.process(pm_output)
            
            # Scrum Master reviews and approves/rejects
            print("✅ Scrum Master: Reviewing for approval")
            approval_output = self.scrum_master.process(task_output)
            
            # Check approval
            approval_tree = etree.fromstring(approval_output.encode())
            approved = approval_tree.find('.//Decision').get('approved') == 'true'
            feedback_text = approval_tree.find('Feedback').text
            
            if approved:
                print(f"\n🎉 PROJECT APPROVED after {iteration} iteration(s)!")
                return {
                    'success': True,
                    'iterations': iteration,
                    'final_output': approval_output,
                    'status': 'APPROVED'
                }
            
            print(f"❌ Project rejected: {feedback_text}")
            
            if iteration >= self.max_iterations:
                print(f"\n💔 PROJECT REJECTED after {self.max_iterations} iterations")
                return {
                    'success': False,
                    'iterations': iteration,
                    'final_output': approval_output,
                    'status': 'REJECTED - Max iterations reached'
                }
            
            # Prepare feedback for Product Manager
            print(f"🔄 Sending feedback to Product Manager for iteration {iteration + 1}")
            current_input = self._create_feedback_xml(feedback_text, analysis_xml)
        
        return {'success': False, 'status': 'Unexpected end'}
    
    def _create_feedback_xml(self, feedback: str, original_analysis: str) -> str:
        """Create feedback XML for Product Manager"""
        analysis_tree = etree.fromstring(original_analysis.encode())
        domain = analysis_tree.find('Domain').text
        content = analysis_tree.find('Content').text
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<FeedbackPacket>
    <Domain>{domain}</Domain>
    <Feedback>{feedback}</Feedback>
    <OriginalContent>{content}</OriginalContent>
</FeedbackPacket>"""


# Example usage
if __name__ == "__main__":
    
    # Complex PRD that will trigger feedback loop
    sample_prd = """
    Enterprise AI Workflow Platform
    
    REQ-001: Real-time visual workflow designer with drag-and-drop interface
    REQ-002: AI-powered document analysis with machine learning models  
    REQ-003: Multi-user collaboration with real-time synchronization
    REQ-004: Enterprise security with SSO and role-based access control
    REQ-005: Advanced analytics dashboard with custom reporting
    REQ-006: API integration layer for third-party connections
    REQ-007: Mobile responsive interface with offline capabilities
    REQ-008: Automated testing framework with CI/CD integration
    REQ-009: Performance monitoring with alerting system
    REQ-010: Data export and backup systems with compliance features
    """
    
    print("🚀 Starting X-Agent Pipeline with PM ↔ Scrum Master Feedback Loop")
    pipeline = XAgentPipeline()
    result = pipeline.execute(sample_prd)
    
    print(f"\n{'='*60}")
    print(f"FINAL RESULT: {result['status']}")
    print(f"Iterations: {result['iterations']}")
    print("="*60)
    
    if result['success']:
        print("✅ Project ready for development!")
    else:
        print("❌ Project needs major revisions")
        
    print("\nFinal approval document:")
    print(result['final_output'])