#!/usr/bin/env python3
"""
X-Agents: XML-MCP Powered Intelligent Workflow Agents
Zero marginal cost, 6ms execution pipeline for enterprise workflow automation
"""

import hashlib
import time
import json
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from lxml import etree
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BaseXAgent(ABC):
    """
    Base class for all X-Agents with XML-MCP integration
    Provides performance tracking, stub patterns, and domain intelligence
    """
    
    def __init__(self, agent_type: str, config: Dict[str, Any] = None):
        self.agent_type = agent_type
        self.config = config or {}
        self.metrics = {
            'parse_time': 0,
            'process_time': 0,
            'total_time': 0,
            'stub_mode': False
        }
        self.patterns = self._load_xml_patterns()
        
    def _load_xml_patterns(self) -> Dict[str, Any]:
        """Load domain-specific XML patterns from schemas"""
        try:
            # Load patterns from requirement_patterns.xml
            patterns = {
                'visual_workflow': ['canvas', 'drag', 'workflow', 'visual'],
                'enterprise': ['compliance', 'security', 'audit', 'governance'],
                'structured': ['database', 'api', 'integration', 'data']
            }
            return patterns
        except Exception as e:
            logger.warning(f"Pattern loading failed, using defaults: {e}")
            return {}
    
    def _generate_stub_response(self, input_data: str, response_type: str = "xml") -> str:
        """Generate deterministic stub responses for development"""
        stub_hash = hashlib.md5(f"{input_data}{self.agent_type}".encode()).hexdigest()[:8]
        
        if response_type == "xml":
            return f"""<?xml version="1.0" encoding="UTF-8"?>
<TaskPacket xmlns="http://xml-mcp.com/schemas/taskpacket/v1.0" stub_mode="true">
    <AgentInfo>
        <Type>{self.agent_type}</Type>
        <StubId>{stub_hash}</StubId>
        <ProcessingTime>6ms</ProcessingTime>
    </AgentInfo>
    <StubData>
        <Message>Stub response for {self.agent_type}</Message>
        <Hash>{stub_hash}</Hash>
    </StubData>
</TaskPacket>"""
        
        return {"stub_mode": True, "agent": self.agent_type, "hash": stub_hash}
    
    def process(self, input_xml: str) -> str:
        """Main processing pipeline with performance tracking"""
        start_time = time.time()
        
        try:
            # Parse input XML
            parse_start = time.time()
            parsed_input = self._parse_xml(input_xml)
            self.metrics['parse_time'] = (time.time() - parse_start) * 1000
            
            # Process with domain intelligence
            process_start = time.time()
            result = self._process_with_intelligence(parsed_input)
            self.metrics['process_time'] = (time.time() - process_start) * 1000
            
            # Generate output XML
            output_xml = self._generate_output_xml(result)
            
        except Exception as e:
            logger.error(f"{self.agent_type} processing error: {e}")
            # Fallback to stub mode
            self.metrics['stub_mode'] = True
            output_xml = self._generate_stub_response(input_xml)
        
        self.metrics['total_time'] = (time.time() - start_time) * 1000
        logger.info(f"{self.agent_type} completed in {self.metrics['total_time']:.2f}ms")
        
        return output_xml
    
    def _parse_xml(self, xml_string: str) -> etree.Element:
        """Parse XML with namespace awareness"""
        try:
            parser = etree.XMLParser(ns_clean=True, recover=True)
            return etree.fromstring(xml_string.encode(), parser)
        except Exception as e:
            logger.error(f"XML parsing error: {e}")
            raise
    
    @abstractmethod
    def _process_with_intelligence(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Abstract method for agent-specific intelligent processing"""
        pass
    
    @abstractmethod
    def _generate_output_xml(self, result: Dict[str, Any]) -> str:
        """Abstract method for generating agent-specific XML output"""
        pass


class AnalystXAgent(BaseXAgent):
    """
    Analyst X-Agent: Document analysis and domain classification
    First agent in the pipeline - determines processing strategy
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("AnalystXAgent", config)
    
    def _process_with_intelligence(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Analyze document and detect domain type"""
        
        # Extract document content
        content = etree.tostring(parsed_input, encoding='unicode', method='text').lower()
        
        # Domain detection using patterns
        domain_scores = {}
        for domain, keywords in self.patterns.items():
            score = sum(1 for keyword in keywords if keyword in content)
            domain_scores[domain] = score
        
        # Determine primary domain
        primary_domain = max(domain_scores, key=domain_scores.get, default='general')
        
        # Extract document metadata
        result = {
            'domain': primary_domain,
            'domain_scores': domain_scores,
            'content_length': len(content),
            'document_type': self._detect_document_type(content),
            'complexity_score': self._calculate_complexity(content),
            'processing_recommendation': self._recommend_processing_strategy(primary_domain)
        }
        
        return result
    
    def _detect_document_type(self, content: str) -> str:
        """Detect specific document type (PRD, spec, requirements, etc.)"""
        doc_types = {
            'prd': ['product requirements', 'product spec', 'feature spec'],
            'technical_spec': ['api spec', 'technical requirements', 'architecture'],
            'user_story': ['user story', 'acceptance criteria', 'scenario'],
            'business_req': ['business requirements', 'business case', 'objectives']
        }
        
        for doc_type, indicators in doc_types.items():
            if any(indicator in content for indicator in indicators):
                return doc_type
        
        return 'general'
    
    def _calculate_complexity(self, content: str) -> int:
        """Calculate document complexity score (1-10)"""
        factors = {
            'length': min(len(content) // 1000, 3),
            'req_count': content.count('req-') + content.count('requirement'),
            'stakeholders': content.count('stakeholder') + content.count('user'),
            'technical_terms': sum(1 for term in ['api', 'database', 'integration', 'service'] if term in content)
        }
        
        return min(sum(factors.values()), 10)
    
    def _recommend_processing_strategy(self, domain: str) -> Dict[str, Any]:
        """Recommend processing strategy based on domain"""
        strategies = {
            'visual_workflow': {
                'agent_sequence': ['ProductManager', 'TaskManager', 'ScrumMaster'],
                'emphasis': 'ui_components',
                'validation_level': 'standard'
            },
            'enterprise': {
                'agent_sequence': ['ProductManager', 'TaskManager', 'ReviewAgent', 'ScrumMaster'],
                'emphasis': 'compliance',
                'validation_level': 'strict'
            },
            'structured': {
                'agent_sequence': ['ProductManager', 'TaskManager'],
                'emphasis': 'data_architecture',
                'validation_level': 'technical'
            }
        }
        
        return strategies.get(domain, strategies['visual_workflow'])
    
    def _generate_output_xml(self, result: Dict[str, Any]) -> str:
        """Generate Analyst XML output for next agent"""
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<AnalysisPacket xmlns="http://xml-mcp.com/schemas/analysis/v1.0">
    <DocumentAnalysis>
        <Domain>{result['domain']}</Domain>
        <DocumentType>{result['document_type']}</DocumentType>
        <ComplexityScore>{result['complexity_score']}</ComplexityScore>
        <ContentLength>{result['content_length']}</ContentLength>
    </DocumentAnalysis>
    <DomainScores>
        {self._format_domain_scores(result['domain_scores'])}
    </DomainScores>
    <ProcessingStrategy>
        <AgentSequence>{','.join(result['processing_recommendation']['agent_sequence'])}</AgentSequence>
        <Emphasis>{result['processing_recommendation']['emphasis']}</Emphasis>
        <ValidationLevel>{result['processing_recommendation']['validation_level']}</ValidationLevel>
    </ProcessingStrategy>
    <Metrics>
        <ParseTime>{self.metrics['parse_time']:.2f}ms</ParseTime>
        <ProcessTime>{self.metrics['process_time']:.2f}ms</ProcessTime>
        <TotalTime>{self.metrics['total_time']:.2f}ms</TotalTime>
    </Metrics>
</AnalysisPacket>"""
    
    def _format_domain_scores(self, scores: Dict[str, int]) -> str:
        """Format domain scores for XML output"""
        score_elements = []
        for domain, score in scores.items():
            score_elements.append(f"<{domain.title()}Score>{score}</{domain.title()}Score>")
        return '\n        '.join(score_elements)


class ProductManagerXAgent(BaseXAgent):
    """
    Product Manager X-Agent: Requirements extraction and stakeholder analysis
    Transforms analysis into structured requirements with business context
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("ProductManagerXAgent", config)
    
    def _process_with_intelligence(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Extract requirements with PM-level strategic analysis"""
        
        # Get analysis context from previous agent
        analysis_context = self._extract_analysis_context(parsed_input)
        content = etree.tostring(parsed_input, encoding='unicode', method='text')
        
        # Extract requirements using domain intelligence
        requirements = self._extract_requirements(content, analysis_context)
        stakeholders = self._identify_stakeholders(content, analysis_context)
        features = self._extract_features(content, requirements)
        objectives = self._extract_business_objectives(content)
        
        result = {
            'requirements': requirements,
            'stakeholders': stakeholders,
            'features': features,
            'objectives': objectives,
            'domain_context': analysis_context,
            'requirement_count': len(requirements),
            'stakeholder_count': len(stakeholders),
            'feature_count': len(features)
        }
        
        return result
    
    def _extract_analysis_context(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Extract context from Analyst agent output"""
        try:
            # Look for AnalysisPacket namespace
            ns = {'analysis': 'http://xml-mcp.com/schemas/analysis/v1.0'}
            
            domain = parsed_input.xpath('//analysis:Domain/text()', namespaces=ns)
            doc_type = parsed_input.xpath('//analysis:DocumentType/text()', namespaces=ns)
            complexity = parsed_input.xpath('//analysis:ComplexityScore/text()', namespaces=ns)
            
            return {
                'domain': domain[0] if domain else 'general',
                'document_type': doc_type[0] if doc_type else 'general',
                'complexity': int(complexity[0]) if complexity else 5
            }
        except Exception:
            return {'domain': 'general', 'document_type': 'general', 'complexity': 5}
    
    def _extract_requirements(self, content: str, context: Dict[str, Any]) -> list:
        """Extract and structure requirements based on domain intelligence"""
        requirements = []
        
        # Pattern-based requirement extraction
        req_patterns = [
            r'REQ-(\d+)[:\s]+(.*?)(?=\n|REQ-|\Z)',
            r'Requirement\s+(\d+)[:\s]+(.*?)(?=\n|Requirement|\Z)',
            r'(\d+\.)\s+(.*?)(?=\n\d+\.|\Z)'
        ]
        
        import re
        for pattern in req_patterns:
            matches = re.finditer(pattern, content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                req_id = match.group(1)
                req_text = match.group(2).strip()
                
                requirement = {
                    'id': f"REQ-{req_id.zfill(3)}",
                    'title': self._extract_requirement_title(req_text),
                    'description': req_text[:200] + '...' if len(req_text) > 200 else req_text,
                    'priority': self._determine_priority(req_text, context),
                    'category': self._categorize_requirement(req_text, context),
                    'complexity': self._estimate_requirement_complexity(req_text)
                }
                requirements.append(requirement)
        
        return requirements[:10]  # Limit to top 10 requirements
    
    def _identify_stakeholders(self, content: str, context: Dict[str, Any]) -> list:
        """Identify stakeholders based on domain and content"""
        stakeholder_indicators = {
            'users': ['user', 'customer', 'end user', 'client'],
            'developers': ['developer', 'engineer', 'dev team', 'technical team'],
            'business': ['business', 'stakeholder', 'product owner', 'pm'],
            'operations': ['ops', 'operations', 'devops', 'admin'],
            'compliance': ['compliance', 'audit', 'security', 'legal']
        }
        
        identified_stakeholders = []
        content_lower = content.lower()
        
        for stakeholder_type, indicators in stakeholder_indicators.items():
            if any(indicator in content_lower for indicator in indicators):
                identified_stakeholders.append({
                    'type': stakeholder_type,
                    'role': stakeholder_type.title(),
                    'importance': self._calculate_stakeholder_importance(stakeholder_type, context),
                    'involvement': self._determine_involvement_level(stakeholder_type, context)
                })
        
        return identified_stakeholders[:5]  # Top 5 stakeholders
    
    def _extract_features(self, content: str, requirements: list) -> list:
        """Extract key features from requirements and content"""
        features = []
        
        # Extract from requirements
        for req in requirements:
            feature_keywords = self._extract_feature_keywords(req['description'])
            if feature_keywords:
                features.append({
                    'name': req['title'],
                    'description': req['description'][:100] + '...',
                    'source_requirement': req['id'],
                    'keywords': feature_keywords,
                    'estimated_effort': self._estimate_feature_effort(req['complexity'])
                })
        
        return features[:6]  # Top 6 features
    
    def _extract_business_objectives(self, content: str) -> list:
        """Extract business objectives and success metrics"""
        objective_patterns = [
            r'objective[s]?[:\s]+(.*?)(?=\n|objective|\Z)',
            r'goal[s]?[:\s]+(.*?)(?=\n|goal|\Z)',
            r'target[s]?[:\s]+(.*?)(?=\n|target|\Z)'
        ]
        
        objectives = []
        import re
        
        for pattern in objective_patterns:
            matches = re.finditer(pattern, content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                obj_text = match.group(1).strip()
                if len(obj_text) > 10:  # Filter out noise
                    objectives.append({
                        'description': obj_text[:150] + '...' if len(obj_text) > 150 else obj_text,
                        'type': self._classify_objective_type(obj_text),
                        'measurable': self._has_metrics(obj_text)
                    })
        
        return objectives[:4]  # Top 4 objectives
    
    def _extract_requirement_title(self, req_text: str) -> str:
        """Extract concise title from requirement text"""
        # Take first sentence or first 50 characters
        sentences = req_text.split('.')
        if sentences:
            title = sentences[0].strip()
            return title[:50] + '...' if len(title) > 50 else title
        return req_text[:50] + '...'
    
    def _determine_priority(self, req_text: str, context: Dict[str, Any]) -> str:
        """Determine requirement priority based on keywords and context"""
        high_priority_keywords = ['critical', 'essential', 'must', 'required', 'core']
        medium_priority_keywords = ['should', 'important', 'preferred']
        
        req_lower = req_text.lower()
        
        if any(keyword in req_lower for keyword in high_priority_keywords):
            return 'high'
        elif any(keyword in req_lower for keyword in medium_priority_keywords):
            return 'medium'
        else:
            return 'low'
    
    def _categorize_requirement(self, req_text: str, context: Dict[str, Any]) -> str:
        """Categorize requirement based on domain and content"""
        categories = {
            'functional': ['feature', 'function', 'capability', 'behavior'],
            'ui_ux': ['interface', 'user experience', 'ui', 'design', 'visual'],
            'performance': ['performance', 'speed', 'response', 'scalability'],
            'security': ['security', 'authentication', 'authorization', 'encryption'],
            'integration': ['integration', 'api', 'connect', 'interface']
        }
        
        req_lower = req_text.lower()
        
        for category, keywords in categories.items():
            if any(keyword in req_lower for keyword in keywords):
                return category
        
        return 'functional'
    
    def _estimate_requirement_complexity(self, req_text: str) -> int:
        """Estimate requirement complexity (1-5)"""
        complexity_indicators = {
            'simple': ['simple', 'basic', 'straightforward'],
            'complex': ['complex', 'advanced', 'sophisticated', 'multiple', 'integration']
        }
        
        req_lower = req_text.lower()
        
        if any(indicator in req_lower for indicator in complexity_indicators['complex']):
            return 4
        elif any(indicator in req_lower for indicator in complexity_indicators['simple']):
            return 2
        else:
            return 3
    
    def _calculate_stakeholder_importance(self, stakeholder_type: str, context: Dict[str, Any]) -> str:
        """Calculate stakeholder importance based on domain context"""
        importance_map = {
            'visual_workflow': {'users': 'high', 'developers': 'high', 'business': 'medium'},
            'enterprise': {'business': 'high', 'compliance': 'high', 'users': 'medium'},
            'structured': {'developers': 'high', 'operations': 'high', 'business': 'medium'}
        }
        
        domain = context.get('domain', 'general')
        domain_map = importance_map.get(domain, {})
        
        return domain_map.get(stakeholder_type, 'medium')
    
    def _determine_involvement_level(self, stakeholder_type: str, context: Dict[str, Any]) -> str:
        """Determine stakeholder involvement level"""
        involvement_map = {
            'users': 'feedback',
            'developers': 'implementation',
            'business': 'decision',
            'operations': 'support',
            'compliance': 'approval'
        }
        
        return involvement_map.get(stakeholder_type, 'consultation')
    
    def _extract_feature_keywords(self, description: str) -> list:
        """Extract key feature keywords"""
        import re
        # Extract nouns and important terms
        words = re.findall(r'\b[a-zA-Z]{4,}\b', description.lower())
        
        # Filter out common words
        common_words = {'that', 'this', 'with', 'from', 'they', 'have', 'will', 'been', 'were'}
        keywords = [word for word in words if word not in common_words]
        
        return keywords[:3]  # Top 3 keywords
    
    def _estimate_feature_effort(self, complexity: int) -> str:
        """Estimate feature development effort"""
        if complexity <= 2:
            return 'small'
        elif complexity <= 3:
            return 'medium'
        else:
            return 'large'
    
    def _classify_objective_type(self, obj_text: str) -> str:
        """Classify business objective type"""
        obj_lower = obj_text.lower()
        
        if any(word in obj_lower for word in ['revenue', 'profit', 'cost', 'roi']):
            return 'financial'
        elif any(word in obj_lower for word in ['user', 'customer', 'satisfaction']):
            return 'customer'
        elif any(word in obj_lower for word in ['efficiency', 'productivity', 'performance']):
            return 'operational'
        else:
            return 'strategic'
    
    def _has_metrics(self, obj_text: str) -> bool:
        """Check if objective has measurable metrics"""
        metric_indicators = ['%', 'percent', 'increase', 'decrease', 'reduce', 'improve', 'target']
        return any(indicator in obj_text.lower() for indicator in metric_indicators)
    
    def _generate_output_xml(self, result: Dict[str, Any]) -> str:
        """Generate Product Manager TaskPacket XML"""
        requirements_xml = self._format_requirements_xml(result['requirements'])
        stakeholders_xml = self._format_stakeholders_xml(result['stakeholders'])
        features_xml = self._format_features_xml(result['features'])
        objectives_xml = self._format_objectives_xml(result['objectives'])
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<TaskPacket xmlns="http://xml-mcp.com/schemas/taskpacket/v1.0">
    <ProjectInfo>
        <Agent>ProductManagerXAgent</Agent>
        <Domain>{result['domain_context']['domain']}</Domain>
        <DocumentType>{result['domain_context']['document_type']}</DocumentType>
        <Complexity>{result['domain_context']['complexity']}</Complexity>
    </ProjectInfo>
    <Requirements count="{result['requirement_count']}">
{requirements_xml}
    </Requirements>
    <Stakeholders count="{result['stakeholder_count']}">
{stakeholders_xml}
    </Stakeholders>
    <Features count="{result['feature_count']}">
{features_xml}
    </Features>
    <BusinessObjectives>
{objectives_xml}
    </BusinessObjectives>
    <Metrics>
        <ParseTime>{self.metrics['parse_time']:.2f}ms</ParseTime>
        <ProcessTime>{self.metrics['process_time']:.2f}ms</ProcessTime>
        <TotalTime>{self.metrics['total_time']:.2f}ms</TotalTime>
    </Metrics>
</TaskPacket>"""
    
    def _format_requirements_xml(self, requirements: list) -> str:
        """Format requirements for XML output"""
        xml_parts = []
        for req in requirements:
            xml_parts.append(f"""        <Requirement id="{req['id']}">
            <Title>{req['title']}</Title>
            <Description>{req['description']}</Description>
            <Priority>{req['priority']}</Priority>
            <Category>{req['category']}</Category>
            <Complexity>{req['complexity']}</Complexity>
        </Requirement>""")
        return '\n'.join(xml_parts)
    
    def _format_stakeholders_xml(self, stakeholders: list) -> str:
        """Format stakeholders for XML output"""
        xml_parts = []
        for stakeholder in stakeholders:
            xml_parts.append(f"""        <Stakeholder type="{stakeholder['type']}">
            <Role>{stakeholder['role']}</Role>
            <Importance>{stakeholder['importance']}</Importance>
            <Involvement>{stakeholder['involvement']}</Involvement>
        </Stakeholder>""")
        return '\n'.join(xml_parts)
    
    def _format_features_xml(self, features: list) -> str:
        """Format features for XML output"""
        xml_parts = []
        for feature in features:
            xml_parts.append(f"""        <Feature>
            <Name>{feature['name']}</Name>
            <Description>{feature['description']}</Description>
            <SourceRequirement>{feature['source_requirement']}</SourceRequirement>
            <EstimatedEffort>{feature['estimated_effort']}</EstimatedEffort>
        </Feature>""")
        return '\n'.join(xml_parts)
    
    def _format_objectives_xml(self, objectives: list) -> str:
        """Format objectives for XML output"""
        xml_parts = []
        for obj in objectives:
            xml_parts.append(f"""        <Objective type="{obj['type']}">
            <Description>{obj['description']}</Description>
            <Measurable>{str(obj['measurable']).lower()}</Measurable>
        </Objective>""")
        return '\n'.join(xml_parts)


class TaskManagerXAgent(BaseXAgent):
    """
    Task Manager X-Agent: Requirements breakdown and task generation
    Transforms requirements into executable development tasks with dependencies
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("TaskManagerXAgent", config)
        self.task_templates = self._load_task_templates()
    
    def _load_task_templates(self) -> Dict[str, Any]:
        """Load task generation templates based on requirement categories"""
        return {
            'functional': {
                'patterns': ['design', 'implement', 'test', 'document'],
                'base_effort': 8,
                'dependencies': ['design_review', 'technical_spec']
            },
            'ui_ux': {
                'patterns': ['wireframe', 'design', 'prototype', 'implement', 'user_test'],
                'base_effort': 6,
                'dependencies': ['design_system', 'user_research']
            },
            'performance': {
                'patterns': ['benchmark', 'optimize', 'test', 'monitor'],
                'base_effort': 12,
                'dependencies': ['performance_baseline', 'monitoring_setup']
            },
            'security': {
                'patterns': ['security_review', 'implement', 'audit', 'document'],
                'base_effort': 10,
                'dependencies': ['security_assessment', 'compliance_check']
            },
            'integration': {
                'patterns': ['api_design', 'implement', 'test', 'document'],
                'base_effort': 16,
                'dependencies': ['api_spec', 'integration_plan']
            }
        }
    
    def _process_with_intelligence(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Break down requirements into executable tasks with 4.6x expansion"""
        
        # Extract requirements from ProductManager output
        requirements = self._extract_requirements_from_taskpacket(parsed_input)
        project_context = self._extract_project_context(parsed_input)
        
        # Generate tasks with intelligent breakdown
        all_tasks = []
        task_id_counter = 1
        
        for req in requirements:
            req_tasks = self._generate_tasks_for_requirement(req, task_id_counter, project_context)
            all_tasks.extend(req_tasks)
            task_id_counter += len(req_tasks)
        
        # Calculate dependencies and estimates
        tasks_with_dependencies = self._calculate_task_dependencies(all_tasks)
        story_points = self._calculate_total_story_points(tasks_with_dependencies)
        
        # Apply domain patterns for additional tasks
        domain_tasks = self._apply_domain_patterns(project_context, task_id_counter)
        all_tasks.extend(domain_tasks)
        
        result = {
            'tasks': tasks_with_dependencies + domain_tasks,
            'total_tasks': len(tasks_with_dependencies) + len(domain_tasks),
            'story_points': story_points + self._calculate_total_story_points(domain_tasks),
            'expansion_ratio': (len(tasks_with_dependencies) + len(domain_tasks)) / max(len(requirements), 1),
            'project_context': project_context,
            'domain_patterns_applied': len(domain_tasks),
            'estimated_duration_weeks': self._estimate_project_duration(story_points + self._calculate_total_story_points(domain_tasks))
        }
        
        return result
    
    def _extract_requirements_from_taskpacket(self, parsed_input: etree.Element) -> list:
        """Extract requirements from ProductManager TaskPacket"""
        requirements = []
        
        try:
            # Navigate TaskPacket XML structure
            ns = {'tp': 'http://xml-mcp.com/schemas/taskpacket/v1.0'}
            req_elements = parsed_input.xpath('//tp:Requirements/tp:Requirement', namespaces=ns)
            
            for req_elem in req_elements:
                requirement = {
                    'id': req_elem.get('id', 'REQ-000'),
                    'title': self._get_element_text(req_elem, 'tp:Title', ns),
                    'description': self._get_element_text(req_elem, 'tp:Description', ns),
                    'priority': self._get_element_text(req_elem, 'tp:Priority', ns),
                    'category': self._get_element_text(req_elem, 'tp:Category', ns),
                    'complexity': int(self._get_element_text(req_elem, 'tp:Complexity', ns, '3'))
                }
                requirements.append(requirement)
                
        except Exception as e:
            logger.warning(f"Failed to extract requirements: {e}")
            # Fallback to stub requirements
            requirements = self._generate_stub_requirements()
        
        return requirements
    
    def _extract_project_context(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Extract project context from TaskPacket"""
        try:
            ns = {'tp': 'http://xml-mcp.com/schemas/taskpacket/v1.0'}
            
            context = {
                'domain': self._get_element_text(parsed_input, '//tp:ProjectInfo/tp:Domain', ns, 'general'),
                'document_type': self._get_element_text(parsed_input, '//tp:ProjectInfo/tp:DocumentType', ns, 'general'),
                'complexity': int(self._get_element_text(parsed_input, '//tp:ProjectInfo/tp:Complexity', ns, '5'))
            }
            
            # Extract stakeholder count for team size estimation
            stakeholder_elements = parsed_input.xpath('//tp:Stakeholders/tp:Stakeholder', namespaces=ns)
            context['team_size_estimate'] = min(len(stakeholder_elements) * 2, 12)  # Estimate team size
            
            return context
            
        except Exception:
            return {'domain': 'general', 'document_type': 'general', 'complexity': 5, 'team_size_estimate': 6}
    
    def _get_element_text(self, element: etree.Element, xpath: str, namespaces: Dict[str, str], default: str = '') -> str:
        """Safely extract text from XML element"""
        try:
            result = element.xpath(xpath, namespaces=namespaces)
            return result[0].text if result and result[0].text else default
        except Exception:
            return default
    
    def _generate_stub_requirements(self) -> list:
        """Generate stub requirements for development mode"""
        return [
            {
                'id': 'REQ-001',
                'title': 'Core Functionality Implementation',
                'description': 'Implement primary system functionality with user interface',
                'priority': 'high',
                'category': 'functional',
                'complexity': 4
            },
            {
                'id': 'REQ-002', 
                'title': 'User Interface Design',
                'description': 'Create responsive user interface with modern design',
                'priority': 'high',
                'category': 'ui_ux',
                'complexity': 3
            }
        ]
    
    def _generate_tasks_for_requirement(self, requirement: Dict[str, Any], start_id: int, context: Dict[str, Any]) -> list:
        """Generate detailed tasks for a single requirement"""
        tasks = []
        category = requirement['category']
        template = self.task_templates.get(category, self.task_templates['functional'])
        
        task_id = start_id
        
        for pattern in template['patterns']:
            task = {
                'id': f"TASK-{task_id:03d}",
                'title': f"{pattern.replace('_', ' ').title()} for {requirement['title']}",
                'description': self._generate_task_description(pattern, requirement),
                'requirement_id': requirement['id'],
                'category': category,
                'pattern': pattern,
                'priority': requirement['priority'],
                'complexity': self._calculate_task_complexity(pattern, requirement['complexity']),
                'estimated_hours': self._estimate_task_hours(pattern, requirement['complexity'], template['base_effort']),
                'story_points': self._calculate_story_points(pattern, requirement['complexity']),
                'dependencies': [],
                'status': 'not_started',
                'assignee': self._suggest_assignee(pattern, category, context)
            }
            tasks.append(task)
            task_id += 1
        
        return tasks
    
    def _generate_task_description(self, pattern: str, requirement: Dict[str, Any]) -> str:
        """Generate detailed task description based on pattern"""
        descriptions = {
            'design': f"Create detailed design specifications for {requirement['title']}. Include wireframes, user flows, and technical architecture.",
            'implement': f"Develop and implement {requirement['title']} according to specifications. Ensure code quality and testing coverage.",
            'test': f"Create comprehensive test suite for {requirement['title']}. Include unit tests, integration tests, and user acceptance criteria.",
            'document': f"Create technical documentation for {requirement['title']}. Include API docs, user guides, and operational procedures.",
            'wireframe': f"Create wireframes and mockups for {requirement['title']}. Include responsive design considerations.",
            'prototype': f"Build interactive prototype for {requirement['title']} to validate user experience and functionality.",
            'optimize': f"Performance optimization for {requirement['title']}. Profile, identify bottlenecks, and implement improvements.",
            'security_review': f"Conduct security assessment for {requirement['title']}. Identify vulnerabilities and implement security measures.",
            'api_design': f"Design API interface for {requirement['title']}. Define endpoints, data models, and integration patterns."
        }
        
        return descriptions.get(pattern, f"Complete {pattern} work for {requirement['title']}")
    
    def _calculate_task_complexity(self, pattern: str, req_complexity: int) -> int:
        """Calculate task complexity based on pattern and requirement"""
        pattern_multipliers = {
            'design': 0.8,
            'implement': 1.2,
            'test': 0.9,
            'document': 0.6,
            'optimize': 1.4,
            'security_review': 1.3,
            'api_design': 1.1
        }
        
        multiplier = pattern_multipliers.get(pattern, 1.0)
        return min(int(req_complexity * multiplier), 5)
    
    def _estimate_task_hours(self, pattern: str, req_complexity: int, base_effort: int) -> int:
        """Estimate hours for task completion"""
        pattern_hours = {
            'design': base_effort * 0.6,
            'implement': base_effort * 1.5,
            'test': base_effort * 0.8,
            'document': base_effort * 0.4,
            'optimize': base_effort * 1.2,
            'security_review': base_effort * 0.7,
            'api_design': base_effort * 0.9
        }
        
        base_hours = pattern_hours.get(pattern, base_effort)
        complexity_multiplier = req_complexity / 3.0
        
        return max(int(base_hours * complexity_multiplier), 2)
    
    def _calculate_story_points(self, pattern: str, req_complexity: int) -> int:
        """Calculate story points using Fibonacci scale"""
        complexity_points = [1, 2, 3, 5, 8]  # Fibonacci sequence
        
        pattern_effort = {
            'design': 2,
            'implement': 5,
            'test': 3,
            'document': 1,
            'optimize': 8,
            'security_review': 5,
            'api_design': 3
        }
        
        base_points = pattern_effort.get(pattern, 3)
        adjusted_points = min(base_points + req_complexity - 3, len(complexity_points) - 1)
        
        return complexity_points[max(adjusted_points, 0)]
    
    def _suggest_assignee(self, pattern: str, category: str, context: Dict[str, Any]) -> str:
        """Suggest assignee based on task pattern and team context"""
        assignee_map = {
            'design': 'UI/UX Designer',
            'implement': 'Senior Developer',
            'test': 'QA Engineer',
            'document': 'Technical Writer',
            'optimize': 'Performance Engineer',
            'security_review': 'Security Engineer',
            'api_design': 'Backend Architect',
            'wireframe': 'UI Designer',
            'prototype': 'Frontend Developer'
        }
        
        # Adjust for team size
        team_size = context.get('team_size_estimate', 6)
        if team_size < 4:
            # Small team - generic roles
            role_mapping = {
                'UI/UX Designer': 'Developer',
                'Technical Writer': 'Developer',
                'Performance Engineer': 'Senior Developer',
                'Security Engineer': 'Senior Developer'
            }
            suggested = assignee_map.get(pattern, 'Developer')
            return role_mapping.get(suggested, suggested)
        
        return assignee_map.get(pattern, 'Developer')
    
    def _calculate_task_dependencies(self, tasks: list) -> list:
        """Calculate dependencies between tasks"""
        # Create dependency map based on patterns
        dependency_rules = {
            'implement': ['design'],
            'test': ['implement'],
            'document': ['implement'],
            'prototype': ['wireframe'],
            'optimize': ['implement', 'test']
        }
        
        # Group tasks by requirement
        req_task_map = {}
        for task in tasks:
            req_id = task['requirement_id']
            if req_id not in req_task_map:
                req_task_map[req_id] = []
            req_task_map[req_id].append(task)
        
        # Calculate dependencies within each requirement
        for req_id, req_tasks in req_task_map.items():
            task_map = {task['pattern']: task for task in req_tasks}
            
            for task in req_tasks:
                pattern = task['pattern']
                if pattern in dependency_rules:
                    for dep_pattern in dependency_rules[pattern]:
                        if dep_pattern in task_map:
                            task['dependencies'].append(task_map[dep_pattern]['id'])
        
        return tasks
    
    def _calculate_total_story_points(self, tasks: list) -> int:
        """Calculate total story points for all tasks"""
        return sum(task['story_points'] for task in tasks)
    
    def _apply_domain_patterns(self, context: Dict[str, Any], start_id: int) -> list:
        """Apply domain-specific patterns to generate additional tasks"""
        domain = context['domain']
        additional_tasks = []
        
        domain_patterns = {
            'visual_workflow': [
                {'pattern': 'ui_framework_setup', 'hours': 16, 'story_points': 5},
                {'pattern': 'drag_drop_implementation', 'hours': 24, 'story_points': 8},
                {'pattern': 'canvas_optimization', 'hours': 12, 'story_points': 5}
            ],
            'enterprise': [
                {'pattern': 'security_audit', 'hours': 20, 'story_points': 8},
                {'pattern': 'compliance_documentation', 'hours': 16, 'story_points': 3},
                {'pattern': 'monitoring_setup', 'hours': 12, 'story_points': 5}
            ],
            'structured': [
                {'pattern': 'database_schema', 'hours': 16, 'story_points': 5},
                {'pattern': 'api_integration', 'hours': 20, 'story_points': 8},
                {'pattern': 'data_migration', 'hours': 24, 'story_points': 8}
            ]
        }
        
        patterns = domain_patterns.get(domain, [])
        task_id = start_id
        
        for pattern_config in patterns:
            task = {
                'id': f"TASK-{task_id:03d}",
                'title': f"Domain Pattern: {pattern_config['pattern'].replace('_', ' ').title()}",
                'description': f"Implement {pattern_config['pattern']} specific to {domain} domain requirements",
                'requirement_id': 'DOMAIN-001',
                'category': 'domain_specific',
                'pattern': pattern_config['pattern'],
                'priority': 'medium',
                'complexity': 4,
                'estimated_hours': pattern_config['hours'],
                'story_points': pattern_config['story_points'],
                'dependencies': [],
                'status': 'not_started',
                'assignee': 'Domain Specialist'
            }
            additional_tasks.append(task)
            task_id += 1
        
        return additional_tasks
    
    def _estimate_project_duration_weeks(self, story_points: int) -> int:
        """Estimate project duration in weeks based on story points"""
        # Assume team velocity of 20-30 story points per sprint (2 weeks)
        average_velocity = 25
        sprints_needed = max(story_points // average_velocity, 1)
        return sprints_needed * 2
    
    def _generate_output_xml(self, result: Dict[str, Any]) -> str:
        """Generate Task Manager TaskPacket XML"""
        tasks_xml = self._format_tasks_xml(result['tasks'])
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<TaskPacket xmlns="http://xml-mcp.com/schemas/taskpacket/v1.0">
    <ProjectInfo>
        <Agent>TaskManagerXAgent</Agent>
        <Domain>{result['project_context']['domain']}</Domain>
        <TeamSize>{result['project_context']['team_size_estimate']}</TeamSize>
    </ProjectInfo>
    <TaskBreakdown>
        <Summary>
            <TotalTasks>{result['total_tasks']}</TotalTasks>
            <StoryPoints>{result['story_points']}</StoryPoints>
            <ExpansionRatio>{result['expansion_ratio']:.1f}x</ExpansionRatio>
            <EstimatedDuration>{result['estimated_duration_weeks']} weeks</EstimatedDuration>
            <DomainPatternsApplied>{result['domain_patterns_applied']}</DomainPatternsApplied>
        </Summary>
        <Tasks count="{result['total_tasks']}">
{tasks_xml}
        </Tasks>
    </TaskBreakdown>
    <Metrics>
        <ParseTime>{self.metrics['parse_time']:.2f}ms</ParseTime>
        <ProcessTime>{self.metrics['process_time']:.2f}ms</ProcessTime>
        <TotalTime>{self.metrics['total_time']:.2f}ms</TotalTime>
    </Metrics>
</TaskPacket>"""
    
    def _format_tasks_xml(self, tasks: list) -> str:
        """Format tasks for XML output"""
        xml_parts = []
        for task in tasks:
            dependencies_xml = ''
            if task['dependencies']:
                dep_list = ', '.join(task['dependencies'])
                dependencies_xml = f'\n            <Dependencies>{dep_list}</Dependencies>'
            
            xml_parts.append(f"""            <Task id="{task['id']}">
                <Title>{task['title']}</Title>
                <Description>{task['description']}</Description>
                <RequirementId>{task['requirement_id']}</RequirementId>
                <Category>{task['category']}</Category>
                <Pattern>{task['pattern']}</Pattern>
                <Priority>{task['priority']}</Priority>
                <Complexity>{task['complexity']}</Complexity>
                <EstimatedHours>{task['estimated_hours']}</EstimatedHours>
                <StoryPoints>{task['story_points']}</StoryPoints>
                <Status>{task['status']}</Status>
                <Assignee>{task['assignee']}</Assignee>{dependencies_xml}
            </Task>""")
        
        return '\n'.join(xml_parts)


class POScrumMasterXAgent(BaseXAgent):
    """
    PO/Scrum Master X-Agent: Final validation and release approval
    Combines Product Owner validation with Scrum Master process oversight
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        super().__init__("POScrumMasterXAgent", config)
        self.quality_gates = self._initialize_quality_gates()
        self.risk_thresholds = self._initialize_risk_thresholds()
    
    def _initialize_quality_gates(self) -> Dict[str, Any]:
        """Initialize quality gate criteria"""
        return {
            'completeness': {
                'min_requirements': 3,
                'min_tasks_per_req': 2,
                'min_story_points': 10
            },
            'feasibility': {
                'max_complexity_avg': 4.0,
                'max_duration_weeks': 24,
                'max_team_size': 15
            },
            'quality': {
                'min_test_coverage_tasks': 0.3,
                'min_documentation_tasks': 0.2,
                'required_patterns': ['design', 'implement', 'test']
            },
            'business_alignment': {
                'min_stakeholder_coverage': 3,
                'required_priorities': ['high'],
                'min_objectives': 1
            }
        }
    
    def _initialize_risk_thresholds(self) -> Dict[str, Any]:
        """Initialize risk assessment thresholds"""
        return {
            'low': {'complexity': 2.5, 'duration': 8, 'team_size': 5},
            'medium': {'complexity': 3.5, 'duration': 16, 'team_size': 10},
            'high': {'complexity': 4.5, 'duration': 24, 'team_size': 15}
        }
    
    def _process_with_intelligence(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Validate project plan and provide release approval decision"""
        
        # Extract project data from TaskManager output
        project_data = self._extract_project_data(parsed_input)
        
        # Run comprehensive validation
        quality_assessment = self._assess_quality_gates(project_data)
        risk_assessment = self._assess_project_risks(project_data)
        stakeholder_validation = self._validate_stakeholder_alignment(project_data)
        resource_assessment = self._assess_resource_requirements(project_data)
        
        # Generate approval decision
        approval_decision = self._generate_approval_decision(
            quality_assessment, risk_assessment, stakeholder_validation, resource_assessment
        )
        
        # Create recommendations
        recommendations = self._generate_recommendations(
            quality_assessment, risk_assessment, project_data
        )
        
        result = {
            'project_data': project_data,
            'quality_assessment': quality_assessment,
            'risk_assessment': risk_assessment,
            'stakeholder_validation': stakeholder_validation,
            'resource_assessment': resource_assessment,
            'approval_decision': approval_decision,
            'recommendations': recommendations,
            'quality_score': self._calculate_overall_quality_score(quality_assessment),
            'risk_level': risk_assessment['overall_risk_level'],
            'go_no_go': approval_decision['approved']
        }
        
        return result
    
    def _extract_project_data(self, parsed_input: etree.Element) -> Dict[str, Any]:
        """Extract comprehensive project data from TaskManager TaskPacket"""
        try:
            ns = {'tp': 'http://xml-mcp.com/schemas/taskpacket/v1.0'}
            
            # Extract summary data
            summary_data = {
                'total_tasks': int(self._get_element_text(parsed_input, '//tp:Summary/tp:TotalTasks', ns, '0')),
                'story_points': int(self._get_element_text(parsed_input, '//tp:Summary/tp:StoryPoints', ns, '0')),
                'expansion_ratio': float(self._get_element_text(parsed_input, '//tp:Summary/tp:ExpansionRatio', ns, '1.0').replace('x', '')),
                'estimated_duration': int(self._get_element_text(parsed_input, '//tp:Summary/tp:EstimatedDuration', ns, '0').replace(' weeks', '')),
                'domain_patterns': int(self._get_element_text(parsed_input, '//tp:Summary/tp:DomainPatternsApplied', ns, '0'))
            }
            
            # Extract tasks
            tasks = []
            task_elements = parsed_input.xpath('//tp:Tasks/tp:Task', namespaces=ns)
            
            for task_elem in task_elements:
                task = {
                    'id': task_elem.get('id', ''),
                    'title': self._get_element_text(task_elem, 'tp:Title', ns),
                    'category': self._get_element_text(task_elem, 'tp:Category', ns),
                    'pattern': self._get_element_text(task_elem, 'tp:Pattern', ns),
                    'priority': self._get_element_text(task_elem, 'tp:Priority', ns),
                    'complexity': int(self._get_element_text(task_elem, 'tp:Complexity', ns, '3')),
                    'story_points': int(self._get_element_text(task_elem, 'tp:StoryPoints', ns, '3')),
                    'estimated_hours': int(self._get_element_text(task_elem, 'tp:EstimatedHours', ns, '8')),
                    'assignee': self._get_element_text(task_elem, 'tp:Assignee', ns),
                    'dependencies': self._get_element_text(task_elem, 'tp:Dependencies', ns, '').split(', ') if self._get_element_text(task_elem, 'tp:Dependencies', ns) else []
                }
                tasks.append(task)
            
            # Extract project context
            project_context = {
                'domain': self._get_element_text(parsed_input, '//tp:ProjectInfo/tp:Domain', ns, 'general'),
                'team_size': int(self._get_element_text(parsed_input, '//tp:ProjectInfo/tp:TeamSize', ns, '6'))
            }
            
            return {
                'summary': summary_data,
                'tasks': tasks,
                'context': project_context
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract project data: {e}")
            return self._generate_stub_project_data()
    
    def _generate_stub_project_data(self) -> Dict[str, Any]:
        """Generate stub project data for development mode"""
        return {
            'summary': {
                'total_tasks': 12,
                'story_points': 45,
                'expansion_ratio': 4.0,
                'estimated_duration': 8,
                'domain_patterns': 3
            },
            'tasks': [
                {
                    'id': 'TASK-001',
                    'title': 'Design Core Interface',
                    'category': 'ui_ux',
                    'pattern': 'design',
                    'priority': 'high',
                    'complexity': 3,
                    'story_points': 5,
                    'estimated_hours': 16,
                    'assignee': 'UI Designer',
                    'dependencies': []
                }
            ],
            'context': {
                'domain': 'visual_workflow',
                'team_size': 6
            }
        }
    
    def _assess_quality_gates(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess project against quality gate criteria"""
        gates = self.quality_gates
        assessment = {}
        
        # Completeness assessment
        completeness = {
            'task_count_adequate': project_data['summary']['total_tasks'] >= gates['completeness']['min_requirements'] * gates['completeness']['min_tasks_per_req'],
            'story_points_adequate': project_data['summary']['story_points'] >= gates['completeness']['min_story_points'],
            'expansion_ratio_healthy': project_data['summary']['expansion_ratio'] >= 3.0,
            'score': 0
        }
        completeness['score'] = sum([completeness['task_count_adequate'], completeness['story_points_adequate'], completeness['expansion_ratio_healthy']]) / 3
        assessment['completeness'] = completeness
        
        # Feasibility assessment
        avg_complexity = sum(task['complexity'] for task in project_data['tasks']) / max(len(project_data['tasks']), 1)
        feasibility = {
            'complexity_manageable': avg_complexity <= gates['feasibility']['max_complexity_avg'],
            'duration_reasonable': project_data['summary']['estimated_duration'] <= gates['feasibility']['max_duration_weeks'],
            'team_size_appropriate': project_data['context']['team_size'] <= gates['feasibility']['max_team_size'],
            'avg_complexity': avg_complexity,
            'score': 0
        }
        feasibility['score'] = sum([feasibility['complexity_manageable'], feasibility['duration_reasonable'], feasibility['team_size_appropriate']]) / 3
        assessment['feasibility'] = feasibility
        
        # Quality assessment
        patterns_present = set(task['pattern'] for task in project_data['tasks'])
        test_tasks = sum(1 for task in project_data['tasks'] if 'test' in task['pattern'])
        doc_tasks = sum(1 for task in project_data['tasks'] if 'document' in task['pattern'])
        
        quality = {
            'has_required_patterns': all(pattern in patterns_present for pattern in gates['quality']['required_patterns']),
            'adequate_test_coverage': test_tasks / max(len(project_data['tasks']), 1) >= gates['quality']['min_test_coverage_tasks'],
            'adequate_documentation': doc_tasks / max(len(project_data['tasks']), 1) >= gates['quality']['min_documentation_tasks'],
            'test_task_ratio': test_tasks / max(len(project_data['tasks']), 1),
            'doc_task_ratio': doc_tasks / max(len(project_data['tasks']), 1),
            'score': 0
        }
        quality['score'] = sum([quality['has_required_patterns'], quality['adequate_test_coverage'], quality['adequate_documentation']]) / 3
        assessment['quality'] = quality
        
        return assessment
    
    def _assess_project_risks(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess project risks and determine overall risk level"""
        
        # Calculate risk factors
        avg_complexity = sum(task['complexity'] for task in project_data['tasks']) / max(len(project_data['tasks']), 1)
        duration_weeks = project_data['summary']['estimated_duration']
        team_size = project_data['context']['team_size']
        
        # Assess individual risk factors
        complexity_risk = self._assess_risk_factor('complexity', avg_complexity)
        duration_risk = self._assess_risk_factor('duration', duration_weeks)
        team_risk = self._assess_risk_factor('team_size', team_size)
        
        # Technical risks
        high_complexity_tasks = sum(1 for task in project_data['tasks'] if task['complexity'] >= 4)
        technical_risk = 'high' if high_complexity_tasks > len(project_data['tasks']) * 0.3 else 'low'
        
        # Dependency risks
        dependent_tasks = sum(1 for task in project_data['tasks'] if task['dependencies'])
        dependency_risk = 'high' if dependent_tasks > len(project_data['tasks']) * 0.5 else 'low'
        
        # Overall risk calculation
        risk_scores = {
            'low': 1,
            'medium': 2,
            'high': 3
        }
        
        total_risk_score = (
            risk_scores[complexity_risk] +
            risk_scores[duration_risk] +
            risk_scores[team_risk] +
            risk_scores[technical_risk] +
            risk_scores[dependency_risk]
        ) / 5
        
        if total_risk_score <= 1.5:
            overall_risk = 'low'
        elif total_risk_score <= 2.5:
            overall_risk = 'medium'
        else:
            overall_risk = 'high'
        
        return {
            'complexity_risk': complexity_risk,
            'duration_risk': duration_risk,
            'team_risk': team_risk,
            'technical_risk': technical_risk,
            'dependency_risk': dependency_risk,
            'overall_risk_level': overall_risk,
            'risk_score': total_risk_score,
            'risk_factors': {
                'avg_complexity': avg_complexity,
                'duration_weeks': duration_weeks,
                'team_size': team_size,
                'high_complexity_tasks': high_complexity_tasks,
                'dependent_tasks': dependent_tasks
            }
        }
    
    def _assess_risk_factor(self, factor: str, value: float) -> str:
        """Assess individual risk factor"""
        thresholds = self.risk_thresholds
        
        if value <= thresholds['low'][factor]:
            return 'low'
        elif value <= thresholds['medium'][factor]:
            return 'medium'
        else:
            return 'high'
    
    def _validate_stakeholder_alignment(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate stakeholder alignment and involvement"""
        
        # Analyze task assignments for stakeholder coverage
        assignees = set(task['assignee'] for task in project_data['tasks'])
        
        # Check for key roles
        key_roles = ['Developer', 'Senior Developer', 'UI Designer', 'QA Engineer']
        role_coverage = sum(1 for role in key_roles if any(role.lower() in assignee.lower() for assignee in assignees))
        
        # Priority task distribution
        high_priority_tasks = sum(1 for task in project_data['tasks'] if task['priority'] == 'high')
        priority_balance = high_priority_tasks / max(len(project_data['tasks']), 1)
        
        validation = {
            'stakeholder_coverage': len(assignees),
            'key_role_coverage': role_coverage,
            'role_coverage_percentage': role_coverage / len(key_roles),
            'priority_balance': priority_balance,
            'priority_balance_healthy': 0.2 <= priority_balance <= 0.5,
            'adequate_coverage': len(assignees) >= 4,
            'score': 0
        }
        
        validation['score'] = (
            (1 if validation['adequate_coverage'] else 0) +
            (validation['role_coverage_percentage']) +
            (1 if validation['priority_balance_healthy'] else 0)
        ) / 3
        
        return validation
    
    def _assess_resource_requirements(self, project_data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess resource requirements and allocation"""
        
        total_hours = sum(task['estimated_hours'] for task in project_data['tasks'])
        team_size = project_data['context']['team_size']
        duration_weeks = project_data['summary']['estimated_duration']
        
        # Calculate resource utilization
        available_hours = team_size * duration_weeks * 40  # 40 hours per person per week
        utilization_rate = total_hours / max(available_hours, 1)
        
        # Analyze skill requirements
        skill_requirements = {}
        for task in project_data['tasks']:
            assignee = task['assignee']
            if assignee not in skill_requirements:
                skill_requirements[assignee] = {'hours': 0, 'tasks': 0}
            skill_requirements[assignee]['hours'] += task['estimated_hours']
            skill_requirements[assignee]['tasks'] += 1
        
        # Check for bottlenecks
        max_individual_hours = max(skill_requirements.values(), key=lambda x: x['hours'], default={'hours': 0})['hours']
        individual_utilization = max_individual_hours / max(duration_weeks * 40, 1)
        
        assessment = {
            'total_estimated_hours': total_hours,
            'available_hours': available_hours,
            'team_utilization_rate': utilization_rate,
            'individual_max_utilization': individual_utilization,
            'skill_requirements': skill_requirements,
            'utilization_healthy': 0.6 <= utilization_rate <= 0.9,
            'no_individual_overload': individual_utilization <= 0.9,
            'resource_balanced': len(skill_requirements) >= team_size * 0.7,
            'score': 0
        }
        
        assessment['score'] = sum([
            assessment['utilization_healthy'],
            assessment['no_individual_overload'],
            assessment['resource_balanced']
        ]) / 3
        
        return assessment
    
    def _generate_approval_decision(self, quality_assessment: Dict[str, Any], risk_assessment: Dict[str, Any], 
                                  stakeholder_validation: Dict[str, Any], resource_assessment: Dict[str, Any]) -> Dict[str, Any]:
        """Generate final approval decision based on all assessments"""
        
        # Calculate component scores
        quality_score = (
            quality_assessment['completeness']['score'] +
            quality_assessment['feasibility']['score'] +
            quality_assessment['quality']['score']
        ) / 3
        
        risk_score = 1 - (risk_assessment['risk_score'] - 1) / 2  # Convert to 0-1 scale
        stakeholder_score = stakeholder_validation['score']
        resource_score = resource_assessment['score']
        
        # Overall project score
        overall_score = (quality_score + risk_score + stakeholder_score + resource_score) / 4
        
        # Approval criteria
        min_score_for_approval = 0.7
        max_risk_for_approval = 'medium'
        
        # Determine approval
        score_approved = overall_score >= min_score_for_approval
        risk_approved = risk_assessment['overall_risk_level'] in ['low', 'medium']
        
        approved = score_approved and risk_approved
        
        # Approval conditions
        conditions = []
        if not score_approved:
            conditions.append(f"Improve overall quality score from {overall_score:.2f} to {min_score_for_approval}")
        
        if not risk_approved:
            conditions.append(f"Reduce project risk from {risk_assessment['overall_risk_level']} to medium or low")
        
        if quality_assessment['quality']['score'] < 0.8:
            conditions.append("Increase test coverage and documentation tasks")
        
        if resource_assessment['team_utilization_rate'] > 0.9:
            conditions.append("Reduce team utilization or extend timeline")
        
        decision = {
            'approved': approved,
            'overall_score': overall_score,
            'component_scores': {
                'quality': quality_score,
                'risk': risk_score,
                'stakeholder': stakeholder_score,
                'resource': resource_score
            },
            'approval_confidence': min(overall_score, 1.0),
            'conditions': conditions,
            'recommendation': self._generate_approval_recommendation(approved, overall_score, conditions)
        }
        
        return decision
    
    def _generate_approval_recommendation(self, approved: bool, score: float, conditions: list) -> str:
        """Generate approval recommendation text"""
        if approved:
            if score >= 0.9:
                return "STRONG APPROVAL: Project exceeds all quality and risk thresholds. Proceed with confidence."
            elif score >= 0.8:
                return "APPROVAL: Project meets all criteria for execution. Monitor progress closely."
            else:
                return "CONDITIONAL APPROVAL: Project is viable but requires attention to identified conditions."
        else:
            if score >= 0.6:
                return "CONDITIONAL REJECTION: Project has potential but requires significant improvements before approval."
            else:
                return "REJECTION: Project does not meet minimum standards for execution. Major revisions required."
    
    def _generate_recommendations(self, quality_assessment: Dict[str, Any], risk_assessment: Dict[str, Any], 
                                project_data: Dict[str, Any]) -> list:
        """Generate actionable recommendations for project improvement"""
        recommendations = []
        
        # Quality-based recommendations
        if quality_assessment['completeness']['score'] < 0.8:
            recommendations.append({
                'category': 'completeness',
                'priority': 'high',
                'description': 'Increase task breakdown granularity to improve project completeness',
                'action': f"Add {int((3.0 - project_data['summary']['expansion_ratio']) * len(project_data['tasks']))} more detailed tasks"
            })
        
        if quality_assessment['quality']['test_task_ratio'] < 0.3:
            recommendations.append({
                'category': 'quality',
                'priority': 'high',
                'description': 'Insufficient test coverage in project plan',
                'action': 'Add comprehensive testing tasks for each functional requirement'
            })
        
        # Risk-based recommendations
        if risk_assessment['overall_risk_level'] == 'high':
            recommendations.append({
                'category': 'risk',
                'priority': 'critical',
                'description': 'Project risk level exceeds acceptable thresholds',
                'action': 'Consider breaking project into smaller phases or reducing scope'
            })
        
        if risk_assessment['technical_risk'] == 'high':
            recommendations.append({
                'category': 'technical',
                'priority': 'high',
                'description': 'High number of complex technical tasks detected',
                'action': 'Add technical risk mitigation tasks and proof-of-concept phases'
            })
        
        # Resource-based recommendations
        if project_data['summary']['estimated_duration'] > 16:
            recommendations.append({
                'category': 'timeline',
                'priority': 'medium',
                'description': 'Project duration exceeds recommended timeline',
                'action': 'Consider parallel workstreams or additional team members'
            })
        
        # Domain-specific recommendations
        domain = project_data['context']['domain']
        if domain == 'visual_workflow' and not any('optimization' in task['pattern'] for task in project_data['tasks']):
            recommendations.append({
                'category': 'domain',
                'priority': 'medium',
                'description': 'Visual workflow projects should include performance optimization',
                'action': 'Add canvas performance optimization and user experience testing tasks'
            })
        
        return recommendations[:6]  # Limit to top 6 recommendations
    
    def _calculate_overall_quality_score(self, quality_assessment: Dict[str, Any]) -> float:
        """Calculate overall quality score (0-100)"""
        total_score = (
            quality_assessment['completeness']['score'] +
            quality_assessment['feasibility']['score'] +
            quality_assessment['quality']['score']
        ) / 3
        
        return round(total_score * 100, 1)
    
    def _generate_output_xml(self, result: Dict[str, Any]) -> str:
        """Generate PO/Scrum Master final approval XML"""
        quality_gates_xml = self._format_quality_gates_xml(result['quality_assessment'])
        risk_assessment_xml = self._format_risk_assessment_xml(result['risk_assessment'])
        approval_decision_xml = self._format_approval_decision_xml(result['approval_decision'])
        recommendations_xml = self._format_recommendations_xml(result['recommendations'])
        
        return f"""<?xml version="1.0" encoding="UTF-8"?>
<ReleaseGatePacket xmlns="http://xml-mcp.com/schemas/releasepacket/v1.0">
    <ProjectValidation>
        <Agent>POScrumMasterXAgent</Agent>
        <Domain>{result['project_data']['context']['domain']}</Domain>
        <OverallQualityScore>{result['quality_score']}</OverallQualityScore>
        <RiskLevel>{result['risk_level']}</RiskLevel>
        <GoNoGo>{str(result['go_no_go']).lower()}</GoNoGo>
    </ProjectValidation>
    <QualityGates>
{quality_gates_xml}
    </QualityGates>
    <RiskAssessment>
{risk_assessment_xml}
    </RiskAssessment>
    <ApprovalDecision>
{approval_decision_xml}
    </ApprovalDecision>
    <Recommendations count="{len(result['recommendations'])}">
{recommendations_xml}
    </Recommendations>
    <Metrics>
        <ParseTime>{self.metrics['parse_time']:.2f}ms</ParseTime>
        <ProcessTime>{self.metrics['process_time']:.2f}ms</ProcessTime>
        <TotalTime>{self.metrics['total_time']:.2f}ms</TotalTime>
    </Metrics>
</ReleaseGatePacket>"""
    
    def _format_quality_gates_xml(self, quality_assessment: Dict[str, Any]) -> str:
        """Format quality gates assessment for XML"""
        gates = []
        
        for gate_name, gate_data in quality_assessment.items():
            gates.append(f"""        <QualityGate name="{gate_name}">
            <Score>{gate_data['score']:.2f}</Score>
            <Status>{'passed' if gate_data['score'] >= 0.7 else 'failed'}</Status>
        </QualityGate>""")
        
        return '\n'.join(gates)
    
    def _format_risk_assessment_xml(self, risk_assessment: Dict[str, Any]) -> str:
        """Format risk assessment for XML"""
        return f"""        <OverallRisk>{risk_assessment['overall_risk_level']}</OverallRisk>
        <RiskScore>{risk_assessment['risk_score']:.2f}</RiskScore>
        <RiskFactors>
            <ComplexityRisk>{risk_assessment['complexity_risk']}</ComplexityRisk>
            <DurationRisk>{risk_assessment['duration_risk']}</DurationRisk>
            <TeamRisk>{risk_assessment['team_risk']}</TeamRisk>
            <TechnicalRisk>{risk_assessment['technical_risk']}</TechnicalRisk>
            <DependencyRisk>{risk_assessment['dependency_risk']}</DependencyRisk>
        </RiskFactors>"""
    
    def _format_approval_decision_xml(self, approval_decision: Dict[str, Any]) -> str:
        """Format approval decision for XML"""
        conditions_xml = ''
        if approval_decision['conditions']:
            condition_items = '\n'.join(f"            <Condition>{condition}</Condition>" 
                                       for condition in approval_decision['conditions'])
            conditions_xml = f"\n        <Conditions>\n{condition_items}\n        </Conditions>"
        
        return f"""        <Approved>{str(approval_decision['approved']).lower()}</Approved>
        <OverallScore>{approval_decision['overall_score']:.2f}</OverallScore>
        <Confidence>{approval_decision['approval_confidence']:.2f}</Confidence>
        <Recommendation>{approval_decision['recommendation']}</Recommendation>{conditions_xml}"""
    
    def _format_recommendations_xml(self, recommendations: list) -> str:
        """Format recommendations for XML"""
        if not recommendations:
            return "        <NoRecommendations>Project meets all standards</NoRecommendations>"
        
        xml_parts = []
        for rec in recommendations:
            xml_parts.append(f"""        <Recommendation category="{rec['category']}" priority="{rec['priority']}">
            <Description>{rec['description']}</Description>
            <Action>{rec['action']}</Action>
        </Recommendation>""")
        
        return '\n'.join(xml_parts)


# Example Usage and Pipeline Orchestrator
class XAgentPipeline:
    """
    X-Agent Pipeline Orchestrator
    Manages the complete 4-agent workflow execution
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.agents = {
            'analyst': AnalystXAgent(config),
            'product_manager': ProductManagerXAgent(config),
            'task_manager': TaskManagerXAgent(config),
            'po_scrum_master': POScrumMasterXAgent(config)
        }
        self.execution_metrics = {}
    
    def execute_pipeline(self, input_document: str) -> Dict[str, Any]:
        """Execute the complete 4-agent pipeline"""
        pipeline_start = time.time()
        results = {}
        
        try:
            # Stage 1: Document Analysis
            logger.info("Starting Analyst X-Agent processing...")
            analysis_result = self.agents['analyst'].process(input_document)
            results['analysis'] = analysis_result
            
            # Stage 2: Product Management
            logger.info("Starting Product Manager X-Agent processing...")
            pm_result = self.agents['product_manager'].process(analysis_result)
            results['product_management'] = pm_result
            
            # Stage 3: Task Management
            logger.info("Starting Task Manager X-Agent processing...")
            task_result = self.agents['task_manager'].process(pm_result)
            results['task_management'] = task_result
            
            # Stage 4: Final Approval
            logger.info("Starting PO/Scrum Master X-Agent processing...")
            approval_result = self.agents['po_scrum_master'].process(task_result)
            results['approval'] = approval_result
            
            # Calculate pipeline metrics
            pipeline_time = (time.time() - pipeline_start) * 1000
            self.execution_metrics = {
                'total_pipeline_time': pipeline_time,
                'agent_times': {
                    'analyst': self.agents['analyst'].metrics['total_time'],
                    'product_manager': self.agents['product_manager'].metrics['total_time'],
                    'task_manager': self.agents['task_manager'].metrics['total_time'],
                    'po_scrum_master': self.agents['po_scrum_master'].metrics['total_time']
                },
                'pipeline_success': True
            }
            
            logger.info(f"Pipeline completed successfully in {pipeline_time:.2f}ms")
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {e}")
            self.execution_metrics = {
                'total_pipeline_time': (time.time() - pipeline_start) * 1000,
                'pipeline_success': False,
                'error': str(e)
            }
            
        return {
            'results': results,
            'metrics': self.execution_metrics,
            'success': self.execution_metrics.get('pipeline_success', False)
        }
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """Get current pipeline status and metrics"""
        return {
            'agents_available': len(self.agents),
            'last_execution_metrics': self.execution_metrics,
            'agent_status': {
                name: {
                    'last_metrics': agent.metrics,
                    'agent_type': agent.agent_type
                }
                for name, agent in self.agents.items()
            }
        }


# Example Flask API Integration
def create_flask_api():
    """Create Flask API for X-Agent pipeline"""
    from flask import Flask, request, jsonify
    
    app = Flask(__name__)
    pipeline = XAgentPipeline()
    
    @app.route('/api/process', methods=['POST'])
    def process_document():
        """Process document through X-Agent pipeline"""
        try:
            input_data = request.get_json()
            document_content = input_data.get('document', '')
            
            if not document_content:
                return jsonify({"error": "No document content provided"}), 400
            
            # Execute pipeline
            result = pipeline.execute_pipeline(document_content)
            
            return jsonify({
                "success": result['success'],
                "results": result['results'],
                "metrics": result['metrics']
            }), 200
            
        except Exception as e:
            return jsonify({"error": str(e)}), 500
    
    @app.route('/api/status', methods=['GET'])
    def get_status():
        """Get pipeline status"""
        status = pipeline.get_pipeline_status()
        return jsonify(status), 200
    
    return app


if __name__ == "__main__":
    # Example usage
    sample_prd = """
    Mobile Task Management App - PRD

    REQ-001: User Authentication System
    Social login integration (Google, Apple, Microsoft)
    Two-factor authentication for enterprise users
    Role-based access control (Admin, Member, Viewer)

    REQ-002: Task Management Interface  
    Create, edit, delete tasks with rich text descriptions
    Priority levels (High, Medium, Low) with color coding
    Due date assignment with calendar integration

    Business Objectives:
    - Increase team productivity by 40%
    - Capture 5% market share in mobile task management
    - Generate $500K ARR within first year
    """
    
    # Initialize pipeline
    pipeline = XAgentPipeline()
    
    # Execute pipeline
    print("🚀 Starting X-Agent Pipeline Execution...")
    result = pipeline.execute_pipeline(sample_prd)
    
    # Display results
    if result['success']:
        print(f"✅ Pipeline completed successfully in {result['metrics']['total_pipeline_time']:.2f}ms")
        print(f"📊 Agent execution times:")
        for agent, time_ms in result['metrics']['agent_times'].items():
            print(f"   - {agent}: {time_ms:.2f}ms")
    else:
        print(f"❌ Pipeline failed: {result['metrics'].get('error', 'Unknown error')}")
    
    # Optional: Start Flask API
    # app = create_flask_api()
    # app.run(host='0.0.0.0', port=5000, debug=True)